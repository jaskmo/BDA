{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression with PyStan\n",
    "\n",
    "TODO: Work in progress \n",
    "\n",
    "Authors: Jonah Gabry, Ben Goodrich, Aki Vehtari, Tuomas Sivula\n",
    "\n",
    "The introduction to Bayesian logistic regression is from a [CRAN vignette](https://cran.r-project.org/web/packages/rstanarm/vignettes/binomial.html) by Jonah Gabry and Ben Goodrich. CRAN vignette was modified to a [R notebook](https://github.com/avehtari/BDA_R_demos/blob/master/demos_rstan/diabetes.Rmd) by Aki Vehtari.  Instead of wells data in CRAN vignette, Pima Indians data is used. The end of the notebook differs significantly from the CRAN vignette.  The R notebook was ported to this Python notebook by Aki Vehtari and Tuomas Sivula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This vignette explains how to estimate generalized linear models (GLMs) for binary (Bernoulli) response variables using PyStan.\n",
    "\n",
    "The four steps of a Bayesian analysis are\n",
    "\n",
    "1. Specify a joint distribution for the outcome(s) and all the unknowns, which typically takes the form of a marginal prior distribution for the unknowns multiplied by a likelihood for the outcome(s) conditional on the unknowns. This joint distribution is proportional to a posterior distribution of the unknowns conditional on the observed data\n",
    "2. Draw from posterior distribution using Markov Chain Monte Carlo (MCMC).\n",
    "3. Evaluate how well the model fits the data and possibly revise the model.\n",
    "4. Draw from the posterior predictive distribution of the outcome(s) given interesting values of the predictors in order to visualize how a manipulation of a predictor affects (a function of) the outcome(s).\n",
    "This notebook demonstrates Steps 1-3 when the likelihood is the product of conditionally independent binomial distributions (possibly with only one trial per observation).\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "For a binomial GLM the likelihood for one observation $y$ can be written as a conditionally binomial PMF $$\\binom{n}{y} \\pi^{y} (1 - \\pi)^{n - y},$$ where $n$ is the known number of trials, $\\pi = g^{-1}(\\eta)$ is the probability of success and $\\eta = \\alpha + \\mathbf{x}^\\top \\boldsymbol{\\beta}$ is a linear predictor. For a sample of size $N$, the likelihood of the entire sample is the product of $N$ individual likelihood contributions.\n",
    "\n",
    "Because $\\pi$ is a probability, for a binomial model the link function $g$ maps between the unit interval (the support of $\\pi$) and the set of all real numbers $\\mathbb{R}$. When applied to a linear predictor $\\eta$ with values in $\\mathbb{R}$, the inverse link function $g^{-1}(\\eta)$ therefore returns a valid probability between 0 and 1.\n",
    "\n",
    "The two most common link functions used for binomial GLMs are the logit and probit functions. With the logit (or log-odds) link function $g(x) = \\ln{\\left(\\frac{x}{1-x}\\right)}$, the likelihood for a single observation becomes\n",
    "\n",
    "$$\\binom{n}{y}\\left(\\text{logit}^{-1}(\\eta)\\right)^y \\left(1 - \\text{logit}^{-1}(\\eta)\\right)^{n-y} = \\binom{n}{y} \\left(\\frac{e^{\\eta}}{1 + e^{\\eta}}\\right)^{y} \\left(\\frac{1}{1 + e^{\\eta}}\\right)^{n - y}$$\n",
    "\n",
    "and the probit link function $g(x) = \\Phi^{-1}(x)$ yields the likelihood\n",
    "\n",
    "$$\\binom{n}{y} \\left(\\Phi(\\eta)\\right)^{y} \\left(1 - \\Phi(\\eta)\\right)^{n - y},$$\n",
    "\n",
    "where $\\Phi$ is the CDF of the standard normal distribution. The differences between the logit and probit functions are minor and -- if, as rstanarm does by default, the probit is scaled so its slope at the origin matches the logit's -- the two link functions should yield similar results. Unless the user has a specific reason to prefer the probit link, we recommend the logit simply because it will be slightly faster and more numerically stable.\n",
    "\n",
    "In theory, there are infinitely many possible link functions, although in practice only a few are typically used. \n",
    "\n",
    "\n",
    "### Priors\n",
    "\n",
    "A full Bayesian analysis requires specifying prior distributions $f(\\alpha)$ and $f(\\boldsymbol{\\beta})$ for the intercept and vector of regression coefficients. \n",
    "\n",
    "As an example, suppose we have $K$ predictors and believe --- prior to seeing the data --- that $\\alpha, \\beta_1, \\dots, \\beta_K$ are as likely to be positive as they are to be negative, but are highly unlikely to be far from zero. These beliefs can be represented by normal distributions with mean zero and a small scale (standard deviation).\n",
    "\n",
    "If, on the other hand, we have less a priori confidence that the parameters will be close to zero then we could use a larger scale for the normal distribution and/or a distribution with heavier tails than the normal like the Student's $t$ distribution.\n",
    "\n",
    "### Posterior\n",
    "\n",
    "With independent prior distributions, the joint posterior distribution for $\\alpha$ and $\\boldsymbol{\\beta}$ is proportional to the product of the priors and the $N$ likelihood contributions:\n",
    "\n",
    "$$f\\left(\\alpha,\\boldsymbol{\\beta} | \\mathbf{y},\\mathbf{X}\\right) \\propto f\\left(\\alpha\\right) \\times \\prod_{k=1}^K f\\left(\\beta_k\\right) \\times \\prod_{i=1}^N { g^{-1}\\left(\\eta_i\\right)^{y_i} \\left(1 - g^{-1}\\left(\\eta_i\\right)\\right)^{n_i-y_i}}.$$\n",
    "\n",
    "This is posterior distribution that PyStan will draw from when using MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Example\n",
    "\n",
    "When the logit link function is used the model is often referred to as a logistic regression model (the inverse logit function is the CDF of the standard logistic distribution). As an example, here we will show how to carry out a analysis for Pima Indians data set similar to analysis from Chapter 5.4 of Gelman and Hill (2007) using PyStan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import stan interface\n",
    "import pystan\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add utilities directory to path\n",
    "import os, sys\n",
    "util_path = os.path.abspath(os.path.join(os.path.pardir, 'utilities_and_data'))\n",
    "if util_path not in sys.path and os.path.exists(util_path):\n",
    "    sys.path.insert(0, util_path)\n",
    "\n",
    "# import from utilities\n",
    "import stan_utility\n",
    "import psis  # pareto smoothed importance sampling\n",
    "import plot_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "First we load and pre-process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36577 entries, 0 to 36576\n",
      "Data columns (total 26 columns):\n",
      "Unnamed: 0              36577 non-null int64\n",
      "subject_id              36577 non-null int64\n",
      "hadm_id                 36577 non-null int64\n",
      "icustay_id              36577 non-null int64\n",
      "gender                  36577 non-null object\n",
      "admittime_hospital      36577 non-null object\n",
      "dischtime_hospital      36577 non-null object\n",
      "los_hospital            36577 non-null float64\n",
      "age                     36577 non-null float64\n",
      "admission_type          36577 non-null object\n",
      "hospital_expire_flag    36577 non-null int64\n",
      "intime_icu              36577 non-null object\n",
      "outtime_icu             36575 non-null object\n",
      "los_icu                 36575 non-null float64\n",
      "first_careunit          36577 non-null object\n",
      "weight                  32891 non-null float64\n",
      "height                  21838 non-null float64\n",
      "apsiii                  36577 non-null int64\n",
      "lods                    36577 non-null int64\n",
      "mlods                   36577 non-null int64\n",
      "oasis                   36577 non-null int64\n",
      "qsofa                   36577 non-null int64\n",
      "saps                    36577 non-null int64\n",
      "sapsii                  36577 non-null int64\n",
      "sirs                    36577 non-null int64\n",
      "sofa                    36577 non-null int64\n",
      "dtypes: float64(5), int64(14), object(7)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_path = os.path.abspath(\n",
    "    os.path.join(\n",
    "        os.path.pardir,\n",
    "        'utilities_and_data',\n",
    "        'dataTable.csv'\n",
    "    )\n",
    ")\n",
    "data = pd.read_csv(data_path)\n",
    "# print some basic info()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>admittime_hospital</th>\n",
       "      <th>dischtime_hospital</th>\n",
       "      <th>los_hospital</th>\n",
       "      <th>age</th>\n",
       "      <th>admission_type</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>intime_icu</th>\n",
       "      <th>outtime_icu</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>first_careunit</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>apsiii</th>\n",
       "      <th>lods</th>\n",
       "      <th>mlods</th>\n",
       "      <th>oasis</th>\n",
       "      <th>qsofa</th>\n",
       "      <th>saps</th>\n",
       "      <th>sapsii</th>\n",
       "      <th>sirs</th>\n",
       "      <th>sofa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>27513</td>\n",
       "      <td>163557</td>\n",
       "      <td>200003</td>\n",
       "      <td>M</td>\n",
       "      <td>2199-08-02 17:02:00</td>\n",
       "      <td>2199-08-22 19:00:00</td>\n",
       "      <td>20.0819</td>\n",
       "      <td>48.2960</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>2199-08-02 19:50:04</td>\n",
       "      <td>2199-08-08 17:09:18</td>\n",
       "      <td>5.8884</td>\n",
       "      <td>SICU</td>\n",
       "      <td>77.0</td>\n",
       "      <td>177.80</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20707</td>\n",
       "      <td>129310</td>\n",
       "      <td>200007</td>\n",
       "      <td>M</td>\n",
       "      <td>2109-02-17 10:02:00</td>\n",
       "      <td>2109-02-20 15:47:00</td>\n",
       "      <td>3.2396</td>\n",
       "      <td>43.3450</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>2109-02-17 10:03:37</td>\n",
       "      <td>2109-02-18 17:03:12</td>\n",
       "      <td>1.2914</td>\n",
       "      <td>CCU</td>\n",
       "      <td>126.0</td>\n",
       "      <td>177.80</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29904</td>\n",
       "      <td>129607</td>\n",
       "      <td>200009</td>\n",
       "      <td>F</td>\n",
       "      <td>2189-11-30 10:45:00</td>\n",
       "      <td>2189-12-06 15:00:00</td>\n",
       "      <td>6.1771</td>\n",
       "      <td>47.5560</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>2189-11-30 10:34:32</td>\n",
       "      <td>2189-12-02 14:17:37</td>\n",
       "      <td>2.1549</td>\n",
       "      <td>CSRU</td>\n",
       "      <td>87.2</td>\n",
       "      <td>161.29</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28448</td>\n",
       "      <td>177527</td>\n",
       "      <td>200012</td>\n",
       "      <td>F</td>\n",
       "      <td>2153-12-23 05:12:00</td>\n",
       "      <td>2153-12-23 15:25:00</td>\n",
       "      <td>0.4257</td>\n",
       "      <td>32.9897</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>2153-12-23 05:12:55</td>\n",
       "      <td>2153-12-23 15:55:54</td>\n",
       "      <td>0.4465</td>\n",
       "      <td>MICU</td>\n",
       "      <td>51.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9514</td>\n",
       "      <td>127229</td>\n",
       "      <td>200014</td>\n",
       "      <td>M</td>\n",
       "      <td>2105-02-16 23:15:00</td>\n",
       "      <td>2105-02-21 13:46:00</td>\n",
       "      <td>4.6049</td>\n",
       "      <td>84.7300</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>2105-02-16 23:16:48</td>\n",
       "      <td>2105-02-18 16:53:29</td>\n",
       "      <td>1.7338</td>\n",
       "      <td>SICU</td>\n",
       "      <td>62.0</td>\n",
       "      <td>167.64</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>21789</td>\n",
       "      <td>112486</td>\n",
       "      <td>200019</td>\n",
       "      <td>F</td>\n",
       "      <td>2178-07-08 09:02:00</td>\n",
       "      <td>2178-07-11 06:45:00</td>\n",
       "      <td>2.9049</td>\n",
       "      <td>82.8831</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>1</td>\n",
       "      <td>2178-07-08 09:03:12</td>\n",
       "      <td>2178-07-11 10:28:40</td>\n",
       "      <td>3.0594</td>\n",
       "      <td>CCU</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>61691</td>\n",
       "      <td>109307</td>\n",
       "      <td>200021</td>\n",
       "      <td>M</td>\n",
       "      <td>2114-12-26 19:44:00</td>\n",
       "      <td>2114-12-28 18:30:00</td>\n",
       "      <td>1.9486</td>\n",
       "      <td>60.8523</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>2114-12-26 19:45:12</td>\n",
       "      <td>2114-12-27 22:46:28</td>\n",
       "      <td>1.1259</td>\n",
       "      <td>SICU</td>\n",
       "      <td>82.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>19167</td>\n",
       "      <td>164161</td>\n",
       "      <td>200025</td>\n",
       "      <td>M</td>\n",
       "      <td>2113-08-25 09:28:00</td>\n",
       "      <td>2113-08-31 19:20:00</td>\n",
       "      <td>6.4111</td>\n",
       "      <td>49.4313</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>0</td>\n",
       "      <td>2113-08-25 09:29:12</td>\n",
       "      <td>2113-08-28 12:42:58</td>\n",
       "      <td>3.1346</td>\n",
       "      <td>CSRU</td>\n",
       "      <td>140.4</td>\n",
       "      <td>193.04</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>41710</td>\n",
       "      <td>181955</td>\n",
       "      <td>200028</td>\n",
       "      <td>M</td>\n",
       "      <td>2133-10-29 10:00:00</td>\n",
       "      <td>2133-11-01 14:54:00</td>\n",
       "      <td>3.2042</td>\n",
       "      <td>64.8677</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>2133-10-29 17:13:50</td>\n",
       "      <td>2133-11-01 14:55:14</td>\n",
       "      <td>2.9038</td>\n",
       "      <td>CCU</td>\n",
       "      <td>84.0</td>\n",
       "      <td>167.64</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>23650</td>\n",
       "      <td>102161</td>\n",
       "      <td>200029</td>\n",
       "      <td>F</td>\n",
       "      <td>2115-03-29 19:59:00</td>\n",
       "      <td>2115-03-29 12:00:00</td>\n",
       "      <td>-0.3326</td>\n",
       "      <td>87.9659</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>1</td>\n",
       "      <td>2115-03-29 20:01:10</td>\n",
       "      <td>2115-03-30 01:13:37</td>\n",
       "      <td>0.2170</td>\n",
       "      <td>CCU</td>\n",
       "      <td>68.0</td>\n",
       "      <td>177.80</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  subject_id  hadm_id  icustay_id gender   admittime_hospital  \\\n",
       "0           0       27513   163557      200003      M  2199-08-02 17:02:00   \n",
       "1           1       20707   129310      200007      M  2109-02-17 10:02:00   \n",
       "2           2       29904   129607      200009      F  2189-11-30 10:45:00   \n",
       "3           3       28448   177527      200012      F  2153-12-23 05:12:00   \n",
       "4           4        9514   127229      200014      M  2105-02-16 23:15:00   \n",
       "5           5       21789   112486      200019      F  2178-07-08 09:02:00   \n",
       "6           6       61691   109307      200021      M  2114-12-26 19:44:00   \n",
       "7           7       19167   164161      200025      M  2113-08-25 09:28:00   \n",
       "8           8       41710   181955      200028      M  2133-10-29 10:00:00   \n",
       "9           9       23650   102161      200029      F  2115-03-29 19:59:00   \n",
       "\n",
       "    dischtime_hospital  los_hospital      age admission_type  \\\n",
       "0  2199-08-22 19:00:00       20.0819  48.2960      EMERGENCY   \n",
       "1  2109-02-20 15:47:00        3.2396  43.3450      EMERGENCY   \n",
       "2  2189-12-06 15:00:00        6.1771  47.5560       ELECTIVE   \n",
       "3  2153-12-23 15:25:00        0.4257  32.9897      EMERGENCY   \n",
       "4  2105-02-21 13:46:00        4.6049  84.7300      EMERGENCY   \n",
       "5  2178-07-11 06:45:00        2.9049  82.8831      EMERGENCY   \n",
       "6  2114-12-28 18:30:00        1.9486  60.8523      EMERGENCY   \n",
       "7  2113-08-31 19:20:00        6.4111  49.4313      EMERGENCY   \n",
       "8  2133-11-01 14:54:00        3.2042  64.8677       ELECTIVE   \n",
       "9  2115-03-29 12:00:00       -0.3326  87.9659      EMERGENCY   \n",
       "\n",
       "   hospital_expire_flag           intime_icu          outtime_icu  los_icu  \\\n",
       "0                     0  2199-08-02 19:50:04  2199-08-08 17:09:18   5.8884   \n",
       "1                     0  2109-02-17 10:03:37  2109-02-18 17:03:12   1.2914   \n",
       "2                     0  2189-11-30 10:34:32  2189-12-02 14:17:37   2.1549   \n",
       "3                     0  2153-12-23 05:12:55  2153-12-23 15:55:54   0.4465   \n",
       "4                     0  2105-02-16 23:16:48  2105-02-18 16:53:29   1.7338   \n",
       "5                     1  2178-07-08 09:03:12  2178-07-11 10:28:40   3.0594   \n",
       "6                     0  2114-12-26 19:45:12  2114-12-27 22:46:28   1.1259   \n",
       "7                     0  2113-08-25 09:29:12  2113-08-28 12:42:58   3.1346   \n",
       "8                     0  2133-10-29 17:13:50  2133-11-01 14:55:14   2.9038   \n",
       "9                     1  2115-03-29 20:01:10  2115-03-30 01:13:37   0.2170   \n",
       "\n",
       "  first_careunit  weight  height  apsiii  lods  mlods  oasis  qsofa  saps  \\\n",
       "0           SICU    77.0  177.80      48     3      2     35      2    24   \n",
       "1            CCU   126.0  177.80      33     2      1     26      2     6   \n",
       "2           CSRU    87.2  161.29      26     4      2     25      2    13   \n",
       "3           MICU    51.2     NaN      21     1      0     25      2     6   \n",
       "4           SICU    62.0  167.64      50     5      2     56      2    23   \n",
       "5            CCU    65.0     NaN      49     7      4     47      2    25   \n",
       "6           SICU    82.6     NaN      48     7      5     41      3    21   \n",
       "7           CSRU   140.4  193.04      21     2      1     30      2    15   \n",
       "8            CCU    84.0  167.64      53     6      4     35      2    20   \n",
       "9            CCU    68.0  177.80      80     9      3     38      2    26   \n",
       "\n",
       "   sapsii  sirs  sofa  \n",
       "0      30     4     6  \n",
       "1      18     2     1  \n",
       "2      21     4     3  \n",
       "3      11     3     1  \n",
       "4      43     4     3  \n",
       "5      54     3     5  \n",
       "6      44     3     4  \n",
       "7      29     4     3  \n",
       "8      39     3    10  \n",
       "9      59     3    11  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview some first rows\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>los_hospital</th>\n",
       "      <th>age</th>\n",
       "      <th>hospital_expire_flag</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>apsiii</th>\n",
       "      <th>lods</th>\n",
       "      <th>mlods</th>\n",
       "      <th>oasis</th>\n",
       "      <th>qsofa</th>\n",
       "      <th>saps</th>\n",
       "      <th>sapsii</th>\n",
       "      <th>sirs</th>\n",
       "      <th>sofa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36575.000000</td>\n",
       "      <td>32891.000000</td>\n",
       "      <td>21838.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "      <td>36577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18288.000000</td>\n",
       "      <td>38187.013806</td>\n",
       "      <td>150014.571889</td>\n",
       "      <td>250157.844684</td>\n",
       "      <td>10.027086</td>\n",
       "      <td>62.289316</td>\n",
       "      <td>0.109003</td>\n",
       "      <td>4.109774</td>\n",
       "      <td>81.909293</td>\n",
       "      <td>169.955572</td>\n",
       "      <td>41.346201</td>\n",
       "      <td>3.869016</td>\n",
       "      <td>2.474205</td>\n",
       "      <td>30.942997</td>\n",
       "      <td>1.694890</td>\n",
       "      <td>17.646144</td>\n",
       "      <td>33.737239</td>\n",
       "      <td>2.730295</td>\n",
       "      <td>3.977308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10559.014735</td>\n",
       "      <td>29359.360533</td>\n",
       "      <td>28930.385510</td>\n",
       "      <td>28903.173477</td>\n",
       "      <td>10.867972</td>\n",
       "      <td>16.949088</td>\n",
       "      <td>0.311647</td>\n",
       "      <td>6.112787</td>\n",
       "      <td>24.913522</td>\n",
       "      <td>13.167715</td>\n",
       "      <td>19.655498</td>\n",
       "      <td>2.681469</td>\n",
       "      <td>2.394321</td>\n",
       "      <td>8.948670</td>\n",
       "      <td>0.730367</td>\n",
       "      <td>5.303262</td>\n",
       "      <td>14.316681</td>\n",
       "      <td>1.020358</td>\n",
       "      <td>3.056666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>100001.000000</td>\n",
       "      <td>200003.000000</td>\n",
       "      <td>-0.843800</td>\n",
       "      <td>14.911100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>101.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9144.000000</td>\n",
       "      <td>13926.000000</td>\n",
       "      <td>124900.000000</td>\n",
       "      <td>225220.000000</td>\n",
       "      <td>4.075000</td>\n",
       "      <td>51.470700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.189100</td>\n",
       "      <td>66.600000</td>\n",
       "      <td>162.560000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18288.000000</td>\n",
       "      <td>27833.000000</td>\n",
       "      <td>150026.000000</td>\n",
       "      <td>250181.000000</td>\n",
       "      <td>6.934700</td>\n",
       "      <td>64.326400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.099600</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>170.180000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>27432.000000</td>\n",
       "      <td>62909.000000</td>\n",
       "      <td>175171.000000</td>\n",
       "      <td>275200.000000</td>\n",
       "      <td>12.002800</td>\n",
       "      <td>76.168100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.115450</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>177.800000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36576.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>199999.000000</td>\n",
       "      <td>299999.000000</td>\n",
       "      <td>294.660400</td>\n",
       "      <td>89.003500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.928000</td>\n",
       "      <td>1251.000000</td>\n",
       "      <td>485.140000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0    subject_id        hadm_id     icustay_id  los_hospital  \\\n",
       "count  36577.000000  36577.000000   36577.000000   36577.000000  36577.000000   \n",
       "mean   18288.000000  38187.013806  150014.571889  250157.844684     10.027086   \n",
       "std    10559.014735  29359.360533   28930.385510   28903.173477     10.867972   \n",
       "min        0.000000      3.000000  100001.000000  200003.000000     -0.843800   \n",
       "25%     9144.000000  13926.000000  124900.000000  225220.000000      4.075000   \n",
       "50%    18288.000000  27833.000000  150026.000000  250181.000000      6.934700   \n",
       "75%    27432.000000  62909.000000  175171.000000  275200.000000     12.002800   \n",
       "max    36576.000000  99999.000000  199999.000000  299999.000000    294.660400   \n",
       "\n",
       "                age  hospital_expire_flag       los_icu        weight  \\\n",
       "count  36577.000000          36577.000000  36575.000000  32891.000000   \n",
       "mean      62.289316              0.109003      4.109774     81.909293   \n",
       "std       16.949088              0.311647      6.112787     24.913522   \n",
       "min       14.911100              0.000000      0.000100      1.000000   \n",
       "25%       51.470700              0.000000      1.189100     66.600000   \n",
       "50%       64.326400              0.000000      2.099600     79.000000   \n",
       "75%       76.168100              0.000000      4.115450     93.000000   \n",
       "max       89.003500              1.000000    153.928000   1251.000000   \n",
       "\n",
       "             height        apsiii          lods         mlods         oasis  \\\n",
       "count  21838.000000  36577.000000  36577.000000  36577.000000  36577.000000   \n",
       "mean     169.955572     41.346201      3.869016      2.474205     30.942997   \n",
       "std       13.167715     19.655498      2.681469      2.394321      8.948670   \n",
       "min      101.600000      0.000000      0.000000      0.000000      3.000000   \n",
       "25%      162.560000     28.000000      2.000000      1.000000     25.000000   \n",
       "50%      170.180000     37.000000      3.000000      2.000000     30.000000   \n",
       "75%      177.800000     50.000000      5.000000      4.000000     37.000000   \n",
       "max      485.140000    185.000000     20.000000     17.000000     70.000000   \n",
       "\n",
       "              qsofa          saps        sapsii          sirs          sofa  \n",
       "count  36577.000000  36577.000000  36577.000000  36577.000000  36577.000000  \n",
       "mean       1.694890     17.646144     33.737239      2.730295      3.977308  \n",
       "std        0.730367      5.303262     14.316681      1.020358      3.056666  \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        1.000000     14.000000     24.000000      2.000000      2.000000  \n",
       "50%        2.000000     17.000000     32.000000      3.000000      3.000000  \n",
       "75%        2.000000     21.000000     42.000000      4.000000      5.000000  \n",
       "max        3.000000     44.000000    118.000000      4.000000     22.000000  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the data column names slightly for easier typing\n",
    "# rename DiabetesPedigreeFunction to dpf\n",
    "\n",
    "# data.rename(columns={'DiabetesPedigreeFunction': 'dpf'}, inplace=True)\n",
    "data['bmi'] = data['weight']/(data['height']/100)**2\n",
    "\n",
    "# make lower\n",
    "data.rename(columns=lambda old_name: old_name.lower(), inplace=True)\n",
    "data.rename(columns={'hospital_expire_flag': 'outcome'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing those observation rows with 0 in selected variables\n",
    "normed_predictors = [\n",
    "    'age',\n",
    "    'los_hospital',\n",
    "    'los_icu',\n",
    "    'mlods',\n",
    "    'oasis', \n",
    "    'qsofa',\n",
    "    'sapsii',\n",
    "    'sirs',\n",
    "]\n",
    "\n",
    "data_TU = data[['age', 'los_icu', 'los_hospital','mlods','oasis', 'qsofa','sapsii','sirs', 'outcome']]\n",
    "\n",
    "data_TU = data_TU[(data[normed_predictors] != 0).all(axis=1)]\n",
    "data_TU = data_TU.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the covariates for easier comparison of coefficient posteriors\n",
    "# N.B. int columns turn into floats\n",
    "data_TU.iloc[:,:-1] -= data.iloc[:,:-1].mean()\n",
    "data_TU.iloc[:,:-1] /= data.iloc[:,:-1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>los_icu</th>\n",
       "      <th>los_hospital</th>\n",
       "      <th>mlods</th>\n",
       "      <th>oasis</th>\n",
       "      <th>qsofa</th>\n",
       "      <th>sapsii</th>\n",
       "      <th>sirs</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.825609</td>\n",
       "      <td>0.290968</td>\n",
       "      <td>0.925178</td>\n",
       "      <td>-0.198054</td>\n",
       "      <td>0.453364</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-0.261041</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.117719</td>\n",
       "      <td>-0.461062</td>\n",
       "      <td>-0.624540</td>\n",
       "      <td>-0.615709</td>\n",
       "      <td>-0.552372</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-1.099224</td>\n",
       "      <td>-0.715725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.869269</td>\n",
       "      <td>-0.319801</td>\n",
       "      <td>-0.354251</td>\n",
       "      <td>-0.198054</td>\n",
       "      <td>-0.664121</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-0.889678</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.324005</td>\n",
       "      <td>-0.388689</td>\n",
       "      <td>-0.498914</td>\n",
       "      <td>-0.198054</td>\n",
       "      <td>2.800081</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.646991</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.215038</td>\n",
       "      <td>-0.171832</td>\n",
       "      <td>-0.655337</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>1.794345</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>1.415325</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.084784</td>\n",
       "      <td>-0.488136</td>\n",
       "      <td>-0.743330</td>\n",
       "      <td>1.054911</td>\n",
       "      <td>1.123855</td>\n",
       "      <td>1.786922</td>\n",
       "      <td>0.716839</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.758626</td>\n",
       "      <td>-0.159530</td>\n",
       "      <td>-0.332719</td>\n",
       "      <td>-0.615709</td>\n",
       "      <td>-0.105378</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-0.330889</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.152125</td>\n",
       "      <td>-0.197287</td>\n",
       "      <td>-0.627797</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>0.453364</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.367596</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.514924</td>\n",
       "      <td>-0.636825</td>\n",
       "      <td>-0.953231</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>0.788609</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>1.764568</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.477679</td>\n",
       "      <td>0.334663</td>\n",
       "      <td>1.024286</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>-0.664121</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-0.470587</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.746145</td>\n",
       "      <td>-0.182646</td>\n",
       "      <td>-0.651187</td>\n",
       "      <td>2.307876</td>\n",
       "      <td>1.682597</td>\n",
       "      <td>1.786922</td>\n",
       "      <td>2.253508</td>\n",
       "      <td>-0.715725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.412157</td>\n",
       "      <td>0.390792</td>\n",
       "      <td>0.205881</td>\n",
       "      <td>-0.198054</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>1.786922</td>\n",
       "      <td>-0.261041</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.979981</td>\n",
       "      <td>1.512997</td>\n",
       "      <td>0.303066</td>\n",
       "      <td>1.054911</td>\n",
       "      <td>2.017842</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>1.205780</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.975682</td>\n",
       "      <td>-0.342818</td>\n",
       "      <td>-0.650295</td>\n",
       "      <td>-0.615709</td>\n",
       "      <td>-0.887618</td>\n",
       "      <td>-0.951426</td>\n",
       "      <td>-1.029375</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.364591</td>\n",
       "      <td>-0.457676</td>\n",
       "      <td>-0.473426</td>\n",
       "      <td>-0.198054</td>\n",
       "      <td>0.118118</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.018353</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.487709</td>\n",
       "      <td>3.698366</td>\n",
       "      <td>1.723460</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>0.341615</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-0.191192</td>\n",
       "      <td>-0.715725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.014699</td>\n",
       "      <td>1.570581</td>\n",
       "      <td>4.083735</td>\n",
       "      <td>2.307876</td>\n",
       "      <td>2.017842</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>1.555023</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.068062</td>\n",
       "      <td>-0.229907</td>\n",
       "      <td>1.698736</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>0.565112</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.367596</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.807181</td>\n",
       "      <td>-0.240999</td>\n",
       "      <td>0.883524</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>0.118118</td>\n",
       "      <td>1.786922</td>\n",
       "      <td>0.437445</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.515038</td>\n",
       "      <td>-0.536134</td>\n",
       "      <td>10.183980</td>\n",
       "      <td>-0.615709</td>\n",
       "      <td>0.229867</td>\n",
       "      <td>-0.951426</td>\n",
       "      <td>-0.610284</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.259884</td>\n",
       "      <td>-0.333559</td>\n",
       "      <td>-0.138405</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>1.570848</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.926385</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.218507</td>\n",
       "      <td>-0.552706</td>\n",
       "      <td>-0.486078</td>\n",
       "      <td>-0.615709</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>-0.951426</td>\n",
       "      <td>1.066082</td>\n",
       "      <td>-0.715725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.371537</td>\n",
       "      <td>-0.144954</td>\n",
       "      <td>0.552782</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>-0.887618</td>\n",
       "      <td>-0.951426</td>\n",
       "      <td>-0.261041</td>\n",
       "      <td>1.244373</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.119244</td>\n",
       "      <td>0.025246</td>\n",
       "      <td>1.181482</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>0.788609</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.577142</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.438329</td>\n",
       "      <td>0.095329</td>\n",
       "      <td>-0.130290</td>\n",
       "      <td>0.637256</td>\n",
       "      <td>1.347351</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>0.646991</td>\n",
       "      <td>-0.715725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>-0.086455</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>0.900358</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>1.066082</td>\n",
       "      <td>-0.715725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.028290</td>\n",
       "      <td>-0.341280</td>\n",
       "      <td>-0.242362</td>\n",
       "      <td>-0.615709</td>\n",
       "      <td>-0.775869</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-0.470587</td>\n",
       "      <td>-0.715725</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.935562</td>\n",
       "      <td>0.780663</td>\n",
       "      <td>-0.125560</td>\n",
       "      <td>1.890221</td>\n",
       "      <td>1.123855</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>1.764568</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.081014</td>\n",
       "      <td>-0.453357</td>\n",
       "      <td>-0.604730</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>-0.775869</td>\n",
       "      <td>-0.951426</td>\n",
       "      <td>0.018353</td>\n",
       "      <td>0.264324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.341140</td>\n",
       "      <td>-0.521002</td>\n",
       "      <td>-0.658217</td>\n",
       "      <td>0.219601</td>\n",
       "      <td>-0.887618</td>\n",
       "      <td>0.417748</td>\n",
       "      <td>-0.540435</td>\n",
       "      <td>-1.695773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age   los_icu  los_hospital     mlods     oasis     qsofa    sapsii  \\\n",
       "0  -0.825609  0.290968      0.925178 -0.198054  0.453364  0.417748 -0.261041   \n",
       "1  -1.117719 -0.461062     -0.624540 -0.615709 -0.552372  0.417748 -1.099224   \n",
       "2  -0.869269 -0.319801     -0.354251 -0.198054 -0.664121  0.417748 -0.889678   \n",
       "4   1.324005 -0.388689     -0.498914 -0.198054  2.800081  0.417748  0.646991   \n",
       "5   1.215038 -0.171832     -0.655337  0.637256  1.794345  0.417748  1.415325   \n",
       "6  -0.084784 -0.488136     -0.743330  1.054911  1.123855  1.786922  0.716839   \n",
       "7  -0.758626 -0.159530     -0.332719 -0.615709 -0.105378  0.417748 -0.330889   \n",
       "8   0.152125 -0.197287     -0.627797  0.637256  0.453364  0.417748  0.367596   \n",
       "9   1.514924 -0.636825     -0.953231  0.219601  0.788609  0.417748  1.764568   \n",
       "10 -0.477679  0.334663      1.024286  0.637256 -0.664121  0.417748 -0.470587   \n",
       "14  0.746145 -0.182646     -0.651187  2.307876  1.682597  1.786922  2.253508   \n",
       "15  0.412157  0.390792      0.205881 -0.198054  0.006370  1.786922 -0.261041   \n",
       "20  0.979981  1.512997      0.303066  1.054911  2.017842  0.417748  1.205780   \n",
       "21 -0.975682 -0.342818     -0.650295 -0.615709 -0.887618 -0.951426 -1.029375   \n",
       "22  0.364591 -0.457676     -0.473426 -0.198054  0.118118  0.417748  0.018353   \n",
       "23 -1.487709  3.698366      1.723460  0.637256  0.341615  0.417748 -0.191192   \n",
       "24 -1.014699  1.570581      4.083735  2.307876  2.017842  0.417748  1.555023   \n",
       "25  0.068062 -0.229907      1.698736  0.637256  0.565112  0.417748  0.367596   \n",
       "26  0.807181 -0.240999      0.883524  0.637256  0.118118  1.786922  0.437445   \n",
       "27 -1.515038 -0.536134     10.183980 -0.615709  0.229867 -0.951426 -0.610284   \n",
       "29  1.259884 -0.333559     -0.138405  0.219601  1.570848  0.417748  0.926385   \n",
       "30  1.218507 -0.552706     -0.486078 -0.615709  0.006370 -0.951426  1.066082   \n",
       "31 -0.371537 -0.144954      0.552782  0.637256 -0.887618 -0.951426 -0.261041   \n",
       "32  0.119244  0.025246      1.181482  0.637256  0.788609  0.417748  0.577142   \n",
       "33  0.438329  0.095329     -0.130290  0.637256  1.347351  0.417748  0.646991   \n",
       "34  0.004170  0.019324     -0.086455  0.219601  0.900358  0.417748  1.066082   \n",
       "35  1.028290 -0.341280     -0.242362 -0.615709 -0.775869  0.417748 -0.470587   \n",
       "36 -0.935562  0.780663     -0.125560  1.890221  1.123855  0.417748  1.764568   \n",
       "40 -0.081014 -0.453357     -0.604730  0.219601 -0.775869 -0.951426  0.018353   \n",
       "41 -0.341140 -0.521002     -0.658217  0.219601 -0.887618  0.417748 -0.540435   \n",
       "\n",
       "        sirs  outcome  \n",
       "0   1.244373        0  \n",
       "1  -0.715725        0  \n",
       "2   1.244373        0  \n",
       "4   1.244373        0  \n",
       "5   0.264324        1  \n",
       "6   0.264324        0  \n",
       "7   1.244373        0  \n",
       "8   0.264324        0  \n",
       "9   0.264324        1  \n",
       "10  1.244373        0  \n",
       "14 -0.715725        0  \n",
       "15  0.264324        0  \n",
       "20  1.244373        0  \n",
       "21  1.244373        0  \n",
       "22  0.264324        0  \n",
       "23 -0.715725        0  \n",
       "24  1.244373        0  \n",
       "25  1.244373        0  \n",
       "26  1.244373        0  \n",
       "27  0.264324        0  \n",
       "29  0.264324        0  \n",
       "30 -0.715725        0  \n",
       "31  1.244373        0  \n",
       "32  0.264324        0  \n",
       "33 -0.715725        0  \n",
       "34 -0.715725        0  \n",
       "35 -0.715725        0  \n",
       "36  0.264324        1  \n",
       "40  0.264324        0  \n",
       "41 -1.695773        0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview some first rows againg\n",
    "data_TU.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preparing the inputs\n",
    "X = data_TU.iloc[:,:-1]\n",
    "y = data_TU.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations = 27850\n",
      "number of predictors = 8\n"
     ]
    }
   ],
   "source": [
    "# get shape into variables\n",
    "n, p = X.shape\n",
    "print('number of observations = {}'.format(n))\n",
    "print('number of predictors = {}'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stan model code for logistic regression\n",
    "\n",
    "Logistic regression with Student's $t$ prior as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/**\n",
      " * Logistic regression t-prior\n",
      " *\n",
      " * Priors:\n",
      " *     weights - student t\n",
      " *     intercept - student t\n",
      " */\n",
      "data {\n",
      "    int<lower=0> n;               // number of data points\n",
      "    int<lower=1> d;               // explanatory variable dimension\n",
      "    matrix[n, d] X;               // explanatory variable\n",
      "    int<lower=0,upper=1> y[n];    // response variable\n",
      "    int<lower=1> p_alpha_df;      // prior degrees of freedom for alpha\n",
      "    real p_alpha_loc;             // prior location for alpha\n",
      "    real<lower=0> p_alpha_scale;  // prior scale for alpha\n",
      "    int<lower=1> p_beta_df;       // prior degrees of freedom for beta\n",
      "    real p_beta_loc;              // prior location for beta\n",
      "    real<lower=0> p_beta_scale;   // prior scale for beta\n",
      "}\n",
      "parameters {\n",
      "    real alpha;      // intercept\n",
      "    vector[d] beta;  // explanatory variable weights\n",
      "}\n",
      "transformed parameters {\n",
      "    // linear predictor\n",
      "    vector[n] eta;\n",
      "    eta = alpha + X * beta;\n",
      "}\n",
      "model {\n",
      "    alpha ~ student_t(p_alpha_df, p_alpha_loc, p_alpha_scale);\n",
      "    beta ~ student_t(p_beta_df, p_beta_loc, p_beta_scale);\n",
      "    y ~ bernoulli_logit(eta);\n",
      "}\n",
      "generated quantities {\n",
      "    vector[n] log_lik;\n",
      "    for (i in 1:n)\n",
      "        log_lik[i] = bernoulli_logit_lpmf(y[i] | eta[i]);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('logistic_t.stan') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_e2861d1751f69db4bf5aa576c9d140e7 NOW.\n"
     ]
    }
   ],
   "source": [
    "model = stan_utility.compile_model('logistic_t.stan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set priors and sample from the posterior\n",
    "\n",
    "Here we'll use a Student t prior with 7 degrees of freedom and a scale of 2.5, which, as discussed above, is a reasonable default prior when coefficients should be close to zero but have some chance of being large. PyStan  returns the posterior distribution for the parameters describing the uncertainty related to unknown parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakob/Documents/DTU/PhD/BDA/BDAenv/lib/python3.5/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-36abf2a1e1d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mp_beta_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfit1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m74749\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msamples1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermuted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DTU/PhD/BDA/BDAenv/lib/python3.5/site-packages/pystan/model.py\u001b[0m in \u001b[0;36msampling\u001b[0;34m(self, data, pars, chains, iter, warmup, thin, seed, init, sample_file, diagnostic_file, verbose, algorithm, control, n_jobs, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mcall_sampler_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mizip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mcall_sampler_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_sampler_star\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m         \u001b[0mret_and_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_sampler_star\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_sampler_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msmpl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret_and_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DTU/PhD/BDA/BDAenv/lib/python3.5/site-packages/pystan/model.py\u001b[0m in \u001b[0;36m_map_parallel\u001b[0;34m(function, args, n_jobs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmap_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "data1 = dict(\n",
    "    n=n,\n",
    "    d=p,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    p_alpha_df=7,\n",
    "    p_alpha_loc=0,\n",
    "    p_alpha_scale=2.5,\n",
    "    p_beta_df=7,\n",
    "    p_beta_loc=0,\n",
    "    p_beta_scale=2.5\n",
    ")\n",
    "fit1 = model.sampling(data=data1, seed=74749)\n",
    "samples1 = fit1.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Inspect the resulting posterior\n",
    "\n",
    "Check n_effs and Rhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>-1.015174</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.143076</td>\n",
       "      <td>-1.299494</td>\n",
       "      <td>-1.109587</td>\n",
       "      <td>-1.014207</td>\n",
       "      <td>-0.919651</td>\n",
       "      <td>-0.744491</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.999538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>0.268131</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.179087</td>\n",
       "      <td>-0.092588</td>\n",
       "      <td>0.145121</td>\n",
       "      <td>0.268793</td>\n",
       "      <td>0.390940</td>\n",
       "      <td>0.617576</td>\n",
       "      <td>2869.0</td>\n",
       "      <td>1.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1]</th>\n",
       "      <td>1.214370</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>0.185477</td>\n",
       "      <td>0.858116</td>\n",
       "      <td>1.087410</td>\n",
       "      <td>1.212147</td>\n",
       "      <td>1.340282</td>\n",
       "      <td>1.577813</td>\n",
       "      <td>2863.0</td>\n",
       "      <td>1.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[2]</th>\n",
       "      <td>-0.018038</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.154842</td>\n",
       "      <td>-0.317233</td>\n",
       "      <td>-0.127057</td>\n",
       "      <td>-0.015590</td>\n",
       "      <td>0.090371</td>\n",
       "      <td>0.279008</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.999856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[3]</th>\n",
       "      <td>0.123833</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.179358</td>\n",
       "      <td>-0.226432</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.127423</td>\n",
       "      <td>0.245617</td>\n",
       "      <td>0.471099</td>\n",
       "      <td>3461.0</td>\n",
       "      <td>0.999971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[4]</th>\n",
       "      <td>-0.097538</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.159845</td>\n",
       "      <td>-0.412036</td>\n",
       "      <td>-0.200400</td>\n",
       "      <td>-0.097875</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.226323</td>\n",
       "      <td>3252.0</td>\n",
       "      <td>0.999609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[5]</th>\n",
       "      <td>0.507957</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.197709</td>\n",
       "      <td>0.136263</td>\n",
       "      <td>0.368516</td>\n",
       "      <td>0.507830</td>\n",
       "      <td>0.640744</td>\n",
       "      <td>0.906786</td>\n",
       "      <td>3162.0</td>\n",
       "      <td>1.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[6]</th>\n",
       "      <td>0.407946</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.149134</td>\n",
       "      <td>0.123293</td>\n",
       "      <td>0.304112</td>\n",
       "      <td>0.404529</td>\n",
       "      <td>0.506626</td>\n",
       "      <td>0.705823</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.999301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[7]</th>\n",
       "      <td>0.356895</td>\n",
       "      <td>0.003518</td>\n",
       "      <td>0.192169</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>0.224827</td>\n",
       "      <td>0.352623</td>\n",
       "      <td>0.481480</td>\n",
       "      <td>0.749368</td>\n",
       "      <td>2984.0</td>\n",
       "      <td>1.001202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean   se_mean        sd      2.5%       25%       50%       75%  \\\n",
       "alpha   -1.015174  0.002262  0.143076 -1.299494 -1.109587 -1.014207 -0.919651   \n",
       "beta[0]  0.268131  0.003343  0.179087 -0.092588  0.145121  0.268793  0.390940   \n",
       "beta[1]  1.214370  0.003466  0.185477  0.858116  1.087410  1.212147  1.340282   \n",
       "beta[2] -0.018038  0.002448  0.154842 -0.317233 -0.127057 -0.015590  0.090371   \n",
       "beta[3]  0.123833  0.003049  0.179358 -0.226432  0.000636  0.127423  0.245617   \n",
       "beta[4] -0.097538  0.002803  0.159845 -0.412036 -0.200400 -0.097875  0.007030   \n",
       "beta[5]  0.507957  0.003516  0.197709  0.136263  0.368516  0.507830  0.640744   \n",
       "beta[6]  0.407946  0.002358  0.149134  0.123293  0.304112  0.404529  0.506626   \n",
       "beta[7]  0.356895  0.003518  0.192169 -0.001486  0.224827  0.352623  0.481480   \n",
       "\n",
       "            97.5%   n_eff      Rhat  \n",
       "alpha   -0.744491  4000.0  0.999538  \n",
       "beta[0]  0.617576  2869.0  1.000258  \n",
       "beta[1]  1.577813  2863.0  1.000793  \n",
       "beta[2]  0.279008  4000.0  0.999856  \n",
       "beta[3]  0.471099  3461.0  0.999971  \n",
       "beta[4]  0.226323  3252.0  0.999609  \n",
       "beta[5]  0.906786  3162.0  1.000351  \n",
       "beta[6]  0.705823  4000.0  0.999301  \n",
       "beta[7]  0.749368  2984.0  1.001202  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print summary of selected variables\n",
    "# use pandas data frame for layout\n",
    "summary = fit1.summary(pars=['alpha', 'beta'])\n",
    "pd.DataFrame(\n",
    "    summary['summary'],\n",
    "    index=summary['summary_rownames'],\n",
    "    columns=summary['summary_colnames']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_effs are high and Rhats<1.1, which is good.\n",
    "\n",
    "Next we check divergences, E-BMFI and treedepth exceedences as explained in [Robust Statistical Workflow with PyStan Case Study](http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html) by Michael Betancourt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 4000 iterations saturated the maximum tree depth of 10 (0.0%)\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n"
     ]
    }
   ],
   "source": [
    "stan_utility.check_treedepth(fit1)\n",
    "stan_utility.check_energy(fit1)\n",
    "stan_utility.check_div(fit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is fine based on these diagnostics and we can proceed with our analysis.\n",
    "\n",
    "Visualise the marginal posterior distributions of each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAJCCAYAAADz36/uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28316b3c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histograms\n",
    "fig, axes = plot_tools.hist_multi_sharex(\n",
    "    [samples1['alpha']] + [sample for sample in samples1['beta'].T],\n",
    "    rowlabels=['intercept'] + list(X.columns),\n",
    "    n_bins=60,\n",
    "    x_lines=0,\n",
    "    figsize=(7, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pareto smoothed importance sampling leave-one-out cross-validation to estimate the predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elpd_loo: -182.3 (SE 12.0)\n"
     ]
    }
   ],
   "source": [
    "loo1, loos1, ks1 = psis.psisloo(samples1['log_lik'])\n",
    "loo1_se = np.sqrt(np.var(loos1, ddof=1)*n)\n",
    "print('elpd_loo: {:.4} (SE {:.3})'.format(loo1, loo1_se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of large (> 0.5) Pareto k estimates\n",
    "np.sum(ks1 > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative horseshoe prior on weights\n",
    "\n",
    "In this example, with $n \\gg p$ the difference is small, and thus we dont expect much difference with a different prior and horseshoe prior is usually more useful for $n<p$.\n",
    "\n",
    "The global scale parameter for horseshoe prior is chosen as recommended by Juho Piironen and Aki Vehtari (2017). On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior. Journal of Machine Learning Research: Workshop and Conference Proceedings (AISTATS 2017 Proceedings), accepted for publication. [arXiv preprint arXiv:1610.05559](http://arxiv.org/abs/1610.05559)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/**\n",
      " * Logistic regression HS-prior\n",
      " *\n",
      " * Priors:\n",
      " *     weights - hierarchical shrinkage\n",
      " *     intercept - student t\n",
      " */\n",
      "\n",
      "data {\n",
      "    int<lower=0> n;                     // number of data points\n",
      "    int<lower=1> d;                     // explanatory variable dimension\n",
      "    matrix[n, d] X;                     // explanatory variable\n",
      "    int<lower=0,upper=1> y[n];          // response variable\n",
      "    int<lower=1> p_alpha_df;            // prior alpha degrees of freedom\n",
      "    real p_alpha_loc;                   // prior alpha location\n",
      "    real<lower=0> p_alpha_scale;        // prior scale alpha\n",
      "    int<lower=1> p_beta_df;             // prior beta degrees of freedom\n",
      "    int<lower=1> p_beta_global_df;      // prior beta global degrees of freedom\n",
      "    real<lower=0> p_beta_global_scale;  // prior beta global scale\n",
      "}\n",
      "\n",
      "parameters {\n",
      "\n",
      "    // intercept\n",
      "    real alpha;\n",
      "\n",
      "    // auxiliary variables for the variance parameters\n",
      "    vector[d] z;\n",
      "    vector<lower=0>[d] lambda_r1;\n",
      "    vector<lower=0>[d] lambda_r2;\n",
      "    real<lower=0> tau_r1;\n",
      "    real<lower=0> tau_r2;\n",
      "}\n",
      "\n",
      "transformed parameters {\n",
      "\n",
      "    vector<lower=0>[d] lambda;  // local variance parameter\n",
      "    real<lower=0> tau;          // global variance parameter\n",
      "    vector[d] beta;             // explanatory variable weights\n",
      "    vector[n] eta;              // linear predictor\n",
      "\n",
      "    lambda = lambda_r1 .* sqrt(lambda_r2);\n",
      "    tau = tau_r1 * sqrt(tau_r2);\n",
      "    beta = z .* (lambda*tau);\n",
      "    eta = alpha + X * beta;\n",
      "}\n",
      "\n",
      "model {\n",
      "\n",
      "    // student t prior for intercept\n",
      "    alpha ~ student_t(p_alpha_df, p_alpha_loc, p_alpha_scale);\n",
      "\n",
      "    z ~ normal(0.0, 1.0);\n",
      "\n",
      "    // half t priors for lambdas\n",
      "    lambda_r1 ~ normal(0.0, 1.0);\n",
      "    lambda_r2 ~ inv_gamma(0.5*p_beta_df, 0.5*p_beta_df);\n",
      "\n",
      "    // half t priors for tau\n",
      "    tau_r1 ~ normal(0.0, p_beta_global_scale);\n",
      "    tau_r2 ~ inv_gamma(0.5*p_beta_global_df, 0.5*p_beta_global_df);\n",
      "\n",
      "    // observation model\n",
      "    y ~ bernoulli_logit(eta);\n",
      "}\n",
      "\n",
      "generated quantities {\n",
      "    vector[n] log_lik;\n",
      "    for (i in 1:n)\n",
      "        log_lik[i] = bernoulli_logit_lpmf(y[i] | eta[i]);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('logistic_hs.stan') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "model = stan_utility.compile_model('logistic_hs.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p0 = 2 # prior guess for the number of relevant variables\n",
    "tau0 = p0 / (p - p0) * 1 / np.sqrt(n)\n",
    "data2 = dict(\n",
    "    n=n,\n",
    "    d=p,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    p_alpha_df=7,\n",
    "    p_alpha_loc=0,\n",
    "    p_alpha_scale=2.5,\n",
    "    p_beta_df=1,\n",
    "    p_beta_global_df=1,\n",
    "    p_beta_global_scale=tau0\n",
    ")\n",
    "fit2 = model.sampling(data=data2, seed=74749)\n",
    "samples2 = fit2.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the horseshoe prior has shrunk the posterior distribution of irrelevant features closer to zero, without affecting the posterior distribution of the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>-0.972157</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.143276</td>\n",
       "      <td>-1.264426</td>\n",
       "      <td>-1.066998</td>\n",
       "      <td>-0.964746</td>\n",
       "      <td>-0.873828</td>\n",
       "      <td>-0.700521</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>0.182342</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.169962</td>\n",
       "      <td>-0.058117</td>\n",
       "      <td>0.039738</td>\n",
       "      <td>0.156690</td>\n",
       "      <td>0.300738</td>\n",
       "      <td>0.561509</td>\n",
       "      <td>2187.0</td>\n",
       "      <td>1.001279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1]</th>\n",
       "      <td>1.134855</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.165206</td>\n",
       "      <td>0.800193</td>\n",
       "      <td>1.023235</td>\n",
       "      <td>1.136612</td>\n",
       "      <td>1.242016</td>\n",
       "      <td>1.468268</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.001648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[2]</th>\n",
       "      <td>0.014218</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.091937</td>\n",
       "      <td>-0.177740</td>\n",
       "      <td>-0.026688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>0.053512</td>\n",
       "      <td>0.227477</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[3]</th>\n",
       "      <td>0.104842</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.147235</td>\n",
       "      <td>-0.107136</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>0.062478</td>\n",
       "      <td>0.186669</td>\n",
       "      <td>0.453316</td>\n",
       "      <td>3225.0</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[4]</th>\n",
       "      <td>-0.024354</td>\n",
       "      <td>0.004711</td>\n",
       "      <td>0.099047</td>\n",
       "      <td>-0.259446</td>\n",
       "      <td>-0.068772</td>\n",
       "      <td>-0.007807</td>\n",
       "      <td>0.024773</td>\n",
       "      <td>0.161351</td>\n",
       "      <td>442.0</td>\n",
       "      <td>1.012295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[5]</th>\n",
       "      <td>0.396166</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>0.189707</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.272083</td>\n",
       "      <td>0.406092</td>\n",
       "      <td>0.528994</td>\n",
       "      <td>0.745738</td>\n",
       "      <td>2256.0</td>\n",
       "      <td>1.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[6]</th>\n",
       "      <td>0.291092</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.161427</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.178404</td>\n",
       "      <td>0.295211</td>\n",
       "      <td>0.401841</td>\n",
       "      <td>0.605812</td>\n",
       "      <td>1705.0</td>\n",
       "      <td>1.003422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[7]</th>\n",
       "      <td>0.321249</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.194694</td>\n",
       "      <td>-0.016229</td>\n",
       "      <td>0.173146</td>\n",
       "      <td>0.327803</td>\n",
       "      <td>0.463665</td>\n",
       "      <td>0.693086</td>\n",
       "      <td>2043.0</td>\n",
       "      <td>1.000677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mean   se_mean        sd      2.5%       25%       50%       75%  \\\n",
       "alpha   -0.972157  0.002265  0.143276 -1.264426 -1.066998 -0.964746 -0.873828   \n",
       "beta[0]  0.182342  0.003634  0.169962 -0.058117  0.039738  0.156690  0.300738   \n",
       "beta[1]  1.134855  0.002612  0.165206  0.800193  1.023235  1.136612  1.242016   \n",
       "beta[2]  0.014218  0.001454  0.091937 -0.177740 -0.026688  0.004477  0.053512   \n",
       "beta[3]  0.104842  0.002593  0.147235 -0.107136  0.002692  0.062478  0.186669   \n",
       "beta[4] -0.024354  0.004711  0.099047 -0.259446 -0.068772 -0.007807  0.024773   \n",
       "beta[5]  0.396166  0.003994  0.189707  0.002694  0.272083  0.406092  0.528994   \n",
       "beta[6]  0.291092  0.003909  0.161427 -0.001565  0.178404  0.295211  0.401841   \n",
       "beta[7]  0.321249  0.004307  0.194694 -0.016229  0.173146  0.327803  0.463665   \n",
       "\n",
       "            97.5%   n_eff      Rhat  \n",
       "alpha   -0.700521  4000.0  1.000846  \n",
       "beta[0]  0.561509  2187.0  1.001279  \n",
       "beta[1]  1.468268  4000.0  1.001648  \n",
       "beta[2]  0.227477  4000.0  1.000275  \n",
       "beta[3]  0.453316  3225.0  0.999200  \n",
       "beta[4]  0.161351   442.0  1.012295  \n",
       "beta[5]  0.745738  2256.0  1.000674  \n",
       "beta[6]  0.605812  1705.0  1.003422  \n",
       "beta[7]  0.693086  2043.0  1.000677  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print summary of selected variables\n",
    "# use pandas data frame for layout\n",
    "summary = fit2.summary(pars=['alpha', 'beta'])\n",
    "pd.DataFrame(\n",
    "    summary['summary'],\n",
    "    index=summary['summary_rownames'],\n",
    "    columns=summary['summary_colnames']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAJCCAYAAADz36/uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDGNJkhozjCVJaswwliSpsf8LzclGbGTwTrUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2831c21860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histograms\n",
    "fig, axes = plot_tools.hist_multi_sharex(\n",
    "    [samples2['alpha']] + [sample for sample in samples2['beta'].T],\n",
    "    rowlabels=['intercept'] + list(X.columns),\n",
    "    n_bins=60,\n",
    "    x_lines=0,\n",
    "    figsize=(7, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute LOO also for the model with Horseshoe prior. Expected log predictive density is higher, but not significantly. This is not surprising as this is a easy data with $n \\gg p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elpd_loo: -181.8 (SE 11.1)\n"
     ]
    }
   ],
   "source": [
    "loo2, loos2, ks2 = psis.psisloo(samples2['log_lik'])\n",
    "loo2_se = np.sqrt(np.var(loos2, ddof=1)*n)\n",
    "print('elpd_loo: {:.4} (SE {:.3})'.format(loo2, loo2_se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of large (> 0.5) Pareto k estimates\n",
    "np.sum(ks2 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elpd_diff: 0.5638 (SE 1.51)\n"
     ]
    }
   ],
   "source": [
    "elpd_diff = loos2 - loos1\n",
    "elpd_diff_se = np.sqrt(np.var(elpd_diff, ddof=1)*n)\n",
    "elpd_diff = np.sum(elpd_diff)\n",
    "print('elpd_diff: {:.4} (SE {:.3})'.format(elpd_diff, elpd_diff_se))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BDA",
   "language": "python",
   "name": "bdaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
